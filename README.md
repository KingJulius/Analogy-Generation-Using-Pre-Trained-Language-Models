# Analogy-Generation-Using-Pre-Trained-Language-Models

## Final Project for ECE-GY 7123 / CS-GY 6953 / Deep Learning

Team Members:

- Julius Dsouza
- Christian Balevski


## Project Goal
To generate analogies by leveraging Natural Language Generation (NLG) models to produce valid target-word explanations with increasing levels of abstraction and analogation through restricting vocabulary sets, akin to the popular party game Taboo.


## Dataset
Our dataset will consist of two parts: (1) word definitions and (2) source words with restricted vocabulary sets.

For word definitions we will be using the OPTED (Online Plain Text English Dictionary), which is based on “The Project Gutenberg Etext of Webster’s Unabridged Dictionary” which is in turn based on the 1913 US Webster’s Unabridged Dictionary. It contains 54,555 words along with their corresponding definitions.

For the second part of our data set, we will be using a list of the 1000 most common English nouns which we have extracted from the British National Corpus, which
includes a large collection of documents in British English. These will be the source words (S). The source words will provide the context for which the NLG model needs
to produce an explanation for. In order to move from explanation to analogy, we will restrict the vocabulary the NLG model can use. The restricted vocabulary set will
consist of both synonyms and collocations for the given source word. For example, for a given source word si ∈ S, s = ”government”.

synonyms = [authority, law, politics, ministry...]

collocations = [central, federal, local, national...]

We chose to include both synonyms and collocations in the restricted vocabulary as we suspect that limiting domain specific terminology will force the model to produce
explanations with increasing levels of abstraction. (Both synonyms and collocations can be sourced from NLTK package in python.) The number of terms included and
ratio of terms between synonyms and collocations in the restricted vocabulary set will be one of the parameters that will be modified during this project to determine optimal performance.


## Model
The OpenAI’s GPT-2 language generation model as the pretrained model has been used for this project. GPT-2 has been shown to be highly effective at generating cogent text
especially through fine tuning. The Fine-Tuned GPT-2 Model is used to generate a number of definitions for the desired word where the definitions serve as candidate analogies words. This results in definitions with the highest confidence similarity. This is then used as an input to our subsequent transformer model which computes the scores for the
synonyms generated by the weights of the previous model. The GPT-2 model learns using the byte pair encoding of the words which stores the context as well as the meaning of the word. This provides us a lot more insight instead of using classical text vectorization techniques such as Term Frequency-Inverse Document Frequency, Information Gain which leads to sparse matrices with higher dimensions, with no semantic information and a strong possibility of overfitting taking place.


## Conclusion
![](/images/image1.png)
Overall, the results of this project were definitely interesting albeit mixed. While the model was able to occasionally produce analogies that appeared human in nature, most
often than not, the analogies were nonsense. Continued research using larger datasets and more scoring and training parameters would be of interest.


## References
[Hope et al. 2017] Hope, T.; Chan, J.; Kittur, A.; and Shahaf, D. 2017. Accelerating innovation through analogy mining. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, 235–243.

[Lofi, El Maarry, and Balke 2013] Lofi, C.; El Maarry, K.; and Balke, W.-T. 2013. Skyline queries in crowdenabled databases. In Proceedings of the 16th International
Conference on Extending Database Technology, 465–476.

[Turney 2005] Turney, P. D. 2005. Measuring semantic similarity by latent relational analysis. arXiv preprint cs/0508053.

[Ushio et al. 2021] Ushio, A.; Espinosa-Anke, L.; Schockaert, S.; and Camacho-Collados, J. 2021. Bert is to nlp what alexnet is to cv: Can pre-trained language models identify analogies? In ACL 2020 Main Conference
